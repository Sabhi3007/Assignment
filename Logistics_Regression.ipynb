{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFZwx4WLACbo"
      },
      "outputs": [],
      "source": [
        "# What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "# Logistic Regression is a statistical method used for binary classification problems, where the output variable is categorical (e.g., yes/no, true/false, 0/1).\n",
        "# It predicts the probability that a given input belongs to a certain category using the logistic (sigmoid) function, which outputs values between 0 and 1.\n",
        "\n",
        "# Differences from Linear Regression:\n",
        "# 1. **Output Type**:\n",
        "#    - Linear Regression predicts continuous values.\n",
        "#    - Logistic Regression predicts probabilities (used for classification).\n",
        "\n",
        "# 2. **Function Used**:\n",
        "#    - Linear Regression uses a linear function.\n",
        "#    - Logistic Regression uses a sigmoid function to map predictions to a range between 0 and 1.\n",
        "\n",
        "# 3. **Application**:\n",
        "#    - Linear Regression is used for regression tasks.\n",
        "#    - Logistic Regression is used for classification tasks.\n",
        "\n",
        "# 4. **Error Measurement**:\n",
        "#    - Linear Regression uses Mean Squared Error (MSE).\n",
        "#    - Logistic Regression uses Log Loss or Cross-Entropy."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z7mrEkVFAWFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z3YBQ0HtAWDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the mathematical equation of Logistic Regression?\n",
        "\n",
        "# The mathematical equation of Logistic Regression is:\n",
        "\n",
        "#               1\n",
        "#   p = -----------------\n",
        "#       1 + e^-(b0 + b1x1 + b2x2 + ... + bnxn)\n",
        "\n",
        "# Where:\n",
        "# - p is the predicted probability of the positive class (output between 0 and 1)\n",
        "# - e is the base of the natural logarithm\n",
        "# - b0 is the intercept (bias)\n",
        "# - b1, b2, ..., bn are the coefficients (weights)\n",
        "# - x1, x2, ..., xn are the input features\n",
        "\n",
        "# This equation uses the sigmoid (logistic) function to map any real-valued number into the range (0, 1)."
      ],
      "metadata": {
        "id": "gYfQTAocAWBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AsgEUu4hAl69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W06_ZpZmAl5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        "\n",
        "# The Sigmoid function is used in Logistic Regression because it converts any real-valued number into a value between 0 and 1.\n",
        "# This is ideal for predicting probabilities in binary classification problems.\n",
        "\n",
        "# Reasons for using the Sigmoid function:\n",
        "# 1. **Probability Output**: It outputs values between 0 and 1, which can be interpreted as probabilities.\n",
        "# 2. **Threshold-Based Classification**: We can apply a threshold (e.g., 0.5) to classify outcomes as class 0 or 1.\n",
        "# 3. **Smooth Gradient**: It has a smooth gradient, which helps in optimizing the weights using gradient descent.\n",
        "# 4. **Mathematical Simplicity**: It is differentiable and has nice mathematical properties for model training.\n",
        "\n",
        "# Sigmoid function formula:\n",
        "#         1\n",
        "#   σ(x) = --------\n",
        "#        1 + e^(-x)"
      ],
      "metadata": {
        "id": "KElV7zlrAl3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3w8FKToLAunm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sWDMMNRzAuli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "\n",
        "# Lasso, Ridge, and Elastic Net are types of regularized linear regression that help prevent overfitting by adding penalty terms to the loss function.\n",
        "\n",
        "# 1. **Ridge Regression (L2 Regularization)**:\n",
        "#    - Adds the sum of the squared coefficients to the loss function.\n",
        "#    - Formula: Loss = RSS + α * Σ(coefficients²)\n",
        "#    - Shrinks coefficients but does not eliminate them.\n",
        "#    - Good when all predictors are expected to have small but non-zero effects.\n",
        "\n",
        "# 2. **Lasso Regression (L1 Regularization)**:\n",
        "#    - Adds the sum of the absolute values of coefficients to the loss function.\n",
        "#    - Formula: Loss = RSS + α * Σ|coefficients|\n",
        "#    - Can shrink some coefficients to zero, effectively performing feature selection.\n",
        "#    - Useful when we expect only a few features to be important.\n",
        "\n",
        "# 3. **Elastic Net Regression**:\n",
        "#    - Combines both L1 and L2 penalties.\n",
        "#    - Formula: Loss = RSS + α * (r * Σ|coefficients| + (1 - r) * Σ(coefficients²))\n",
        "#    - Balances between Ridge and Lasso.\n",
        "#    - Useful when there are multiple correlated features or when some coefficients should be exactly zero."
      ],
      "metadata": {
        "id": "R1lxhXjoAujX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "29iJZRWBA7h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SWQzHcXHA7gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m5SXYqb6A7eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "\n",
        "\n",
        "# In Logistic Regression, the regularization parameter (λ), often represented as `C` in libraries like scikit-learn (where C = 1/λ), controls the strength of regularization.\n",
        "\n",
        "# Impact of λ (lambda):\n",
        "# 1. **λ High (Strong Regularization)**:\n",
        "#    - Penalizes large coefficients more heavily.\n",
        "#    - Encourages simpler models.\n",
        "#    - Reduces overfitting.\n",
        "#    - Might lead to underfitting if too strong.\n",
        "\n",
        "# 2. **λ Low (Weak Regularization)**:\n",
        "#    - Coefficients are less penalized.\n",
        "#    - Model becomes more complex and flexible.\n",
        "#    - May fit the training data better but risk overfitting.\n",
        "\n",
        "# 3. **λ = 0**:\n",
        "#    - No regularization is applied.\n",
        "#    - Model behaves like standard logistic regression without penalty.\n"
      ],
      "metadata": {
        "id": "TMt66Ff4A7b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NMAeOq_tBGkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l3WEWfchBHLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JzRukIt3BHJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ck8UonbDBHIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        "\n",
        "# 1. **Decision Trees**\n",
        "#    - Splits data based on feature values.\n",
        "#    - Easy to interpret and visualize.\n",
        "\n",
        "# 2. **Random Forest**\n",
        "#    - Ensemble of decision trees.\n",
        "#    - More accurate and reduces overfitting.\n",
        "\n",
        "# 3. **Support Vector Machines (SVM)**\n",
        "#    - Finds the best boundary (hyperplane) that separates classes.\n",
        "#    - Effective in high-dimensional spaces.\n",
        "\n",
        "# 4. **K-Nearest Neighbors (KNN)**\n",
        "#    - Classifies based on the majority label of nearby data points.\n",
        "#    - Simple and non-parametric.\n",
        "\n",
        "# 5. **Naive Bayes**\n",
        "#    - Based on Bayes’ Theorem.\n",
        "#    - Works well with text classification and high-dimensional data.\n",
        "\n",
        "# 6. **Gradient Boosting (e.g., XGBoost, LightGBM)**\n",
        "#    - Builds strong classifiers from weak learners.\n",
        "#    - High performance in many real-world tasks.\n",
        "\n",
        "# 7. **Neural Networks**\n",
        "#    - Especially useful for complex patterns and large datasets.\n",
        "#    - Backbone of deep learning models."
      ],
      "metadata": {
        "id": "FsMvwi7wBHGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YZTw8a0vBYeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D53UmB0bBYcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  What are Classification Evaluation Metrics?\n",
        "\n",
        "\n",
        "# 1. **Accuracy**:\n",
        "#    - Proportion of correctly predicted instances.\n",
        "#    - Formula: (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "# 2. **Precision**:\n",
        "#    - How many predicted positives are actually positive.\n",
        "#    - Formula: TP / (TP + FP)\n",
        "\n",
        "# 3. **Recall (Sensitivity or True Positive Rate)**:\n",
        "#    - How many actual positives were correctly predicted.\n",
        "#    - Formula: TP / (TP + FN)\n",
        "\n",
        "# 4. **F1 Score**:\n",
        "#    - Harmonic mean of Precision and Recall.\n",
        "#    - Formula: 2 * (Precision * Recall) / (Precision + Recall)\n",
        "\n",
        "# 5. **Confusion Matrix**:\n",
        "#    - Table showing TP, TN, FP, FN.\n",
        "#    - Helps visualize model performance.\n",
        "\n",
        "# 6. **ROC Curve (Receiver Operating Characteristic)**:\n",
        "#    - Plots True Positive Rate vs False Positive Rate.\n",
        "\n",
        "# 7. **AUC (Area Under the Curve)**:\n",
        "#    - Measures overall ability to distinguish between classes.\n",
        "\n",
        "# 8. **Log Loss (Cross-Entropy Loss)**:\n",
        "#    - Measures prediction uncertainty.\n",
        "#    - Penalizes wrong predictions with high confidence."
      ],
      "metadata": {
        "id": "dIkv6uFBBYaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VQt7M5b7Bmrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xyBKl63EBmpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "# Hyperparameter Tuning in Logistic Regression involves selecting the best values for model parameters\n",
        "# that are not learned from the data but set before the training process.\n",
        "\n",
        "# Common hyperparameters in Logistic Regression:\n",
        "# 1. **C (Inverse of regularization strength)**:\n",
        "#    - Smaller C implies stronger regularization.\n",
        "#    - Helps prevent overfitting.\n",
        "\n",
        "# 2. **Penalty**:\n",
        "#    - Type of regularization: 'l1' (Lasso), 'l2' (Ridge), or 'elasticnet'.\n",
        "#    - Controls the impact of feature selection and shrinkage.\n",
        "\n",
        "# 3. **Solver**:\n",
        "#    - Algorithm used for optimization (e.g., 'liblinear', 'saga', 'lbfgs').\n",
        "#    - Some solvers support only certain types of penalties.\n",
        "\n",
        "# 4. **Max_iter**:\n",
        "#    - Maximum number of iterations for the solver to converge.\n",
        "\n",
        "# Methods for Hyperparameter Tuning:\n",
        "# - **Grid Search**: Tests all combinations of parameters from a specified grid.\n",
        "# - **Random Search**: Randomly selects combinations to test, faster than grid search.\n",
        "# - **Bayesian Optimization / Automated ML tools**: More advanced and efficient."
      ],
      "metadata": {
        "id": "xh-i-uX1BnEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_N0KwByqB16U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "erjhfQQtB136"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5X9kgmhGB12L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "\n",
        "# 1. **One-vs-Rest (OvR) or One-vs-All (OvA)**:\n",
        "#    - Train one binary classifier per class.\n",
        "#    - Each classifier distinguishes one class vs all others.\n",
        "#    - During prediction, the class with the highest probability score is selected.\n",
        "\n",
        "# 2. **Multinomial Logistic Regression (Softmax Regression)**:\n",
        "#    - Generalizes logistic regression by modeling probabilities using the softmax function.\n",
        "#    - Predicts the probability of each class directly in a single model.\n",
        "#    - Suitable when classes are mutually exclusive.\n",
        "\n",
        "# Most libraries like scikit-learn implement both; the multinomial option is often preferred for better accuracy.\n",
        "\n",
        "# Softmax function formula:\n",
        "# For class k:\n",
        "#     P(y=k) = exp(θ_k · x) / Σ exp(θ_j · x)  for all classes j"
      ],
      "metadata": {
        "id": "kFCnw8tFB10Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L3OM5cWgCKNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DSIjyQy8CKuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What are some use cases of Logistic Regression?\n",
        "\n",
        "# 1. **Binary Classification Problems**:\n",
        "#    - Predicting whether an email is spam or not.\n",
        "#    - Predicting if a customer will churn or stay.\n",
        "\n",
        "# 2. **Medical Diagnosis**:\n",
        "#    - Predicting presence or absence of a disease.\n",
        "#    - Classifying patients as high-risk or low-risk.\n",
        "\n",
        "# 3. **Credit Scoring**:\n",
        "#    - Determining whether a loan applicant is likely to default.\n",
        "\n",
        "# 4. **Marketing**:\n",
        "#    - Predicting if a user will click on an advertisement (click-through rate).\n",
        "\n",
        "# 5. **Customer Segmentation**:\n",
        "#    - Classifying customers into groups based on purchasing behavior.\n",
        "\n",
        "# 6. **Fraud Detection**:\n",
        "#    - Identifying fraudulent transactions.\n",
        "\n",
        "# Logistic Regression is favored for its interpretability and efficiency, especially when relationships are roughly linear and features are not overly complex."
      ],
      "metadata": {
        "id": "8xqtPnc6CKrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iZITxgCSCWcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "60R4Xwi7CWZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "\n",
        "# 1. **Purpose**:\n",
        "#    - Logistic Regression is used for **binary classification** (two classes).\n",
        "#    - Softmax Regression (Multinomial Logistic Regression) is used for **multiclass classification** (three or more classes).\n",
        "\n",
        "# 2. **Output**:\n",
        "#    - Logistic Regression outputs the probability of one class (class 1) using the sigmoid function.\n",
        "#    - Softmax Regression outputs probabilities for all classes using the softmax function, ensuring they sum to 1.\n",
        "\n",
        "# 3. **Mathematical Function**:\n",
        "#    - Logistic Regression uses the sigmoid function:\n",
        "#      sigmoid(z) = 1 / (1 + exp(-z))\n",
        "#    - Softmax Regression uses the softmax function:\n",
        "#      P(y=k) = exp(θ_k · x) / Σ exp(θ_j · x) for all classes j\n",
        "\n",
        "# 4. **Model Complexity**:\n",
        "#    - Logistic Regression fits one decision boundary.\n",
        "#    - Softmax Regression fits multiple decision boundaries, one per class.\n",
        "\n",
        "# 5. **Implementation**:\n",
        "#    - Logistic Regression is a special case of Softmax Regression when there are only two classes."
      ],
      "metadata": {
        "id": "X8kUPl6_CW8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z4-eyWgqCg0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DHoV2RenCgxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "# Choosing between One-vs-Rest (OvR) and Softmax (Multinomial Logistic Regression) depends on several factors:\n",
        "\n",
        "# 1. **Number of Classes**:\n",
        "#    - OvR can be simpler and faster when there are many classes.\n",
        "#    - Softmax is often better when the number of classes is moderate.\n",
        "\n",
        "# 2. **Class Imbalance**:\n",
        "#    - OvR can handle imbalanced classes better by training separate binary classifiers.\n",
        "#    - Softmax models all classes jointly, which might be sensitive to imbalance.\n",
        "\n",
        "# 3. **Performance**:\n",
        "#    - Softmax often provides better calibrated probabilities and overall accuracy.\n",
        "#    - OvR might be less accurate if classes are not well separated.\n",
        "\n",
        "# 4. **Interpretability**:\n",
        "#    - OvR gives separate binary classifiers, easier to interpret individually.\n",
        "#    - Softmax models the probability distribution across all classes at once.\n",
        "\n",
        "# 5. **Computational Resources**:\n",
        "#    - OvR trains multiple independent models, which can be parallelized.\n",
        "#    - Softmax trains a single model but might be slower for many classes.\n",
        "\n",
        "# In practice:\n",
        "# - Use Softmax if you want a single, well-calibrated model and classes are balanced.\n",
        "# - Use OvR if you need simpler or faster training or handle severe class imbalance."
      ],
      "metadata": {
        "id": "w66oXNrwChQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sZBMly2-C9B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nNJQBYGGC9AK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "# Interpretation of coefficients in Logistic Regression:\n",
        "\n",
        "# - Each coefficient represents the change in the log-odds of the outcome for a one-unit increase in the corresponding feature, holding other features constant.\n",
        "\n",
        "# - Log-odds: The logarithm of the odds (probability of event / probability of no event).\n",
        "\n",
        "# - Positive coefficient: Increases the log-odds, meaning higher chance of the event (class=1).\n",
        "# - Negative coefficient: Decreases the log-odds, meaning lower chance of the event.\n",
        "\n",
        "# - To interpret in terms of odds ratio:\n",
        "#     odds_ratio = exp(coefficient)\n",
        "#     - If odds_ratio > 1, feature increases odds of the event.\n",
        "#     - If odds_ratio < 1, feature decreases odds of the event.\n",
        "\n",
        "# Example:\n",
        "# Coefficient = 0.5\n",
        "# odds_ratio = exp(0.5) ≈ 1.65\n",
        "# Meaning: A one-unit increase in the feature multiplies the odds of the event by 1.65 (65% increase).\n",
        "\n",
        "# For multiple features, interpret coefficients individually while keeping other variables constant."
      ],
      "metadata": {
        "id": "uP1gi7v0C89y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HcCi_H0yDGvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uOP4YmHbDGtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Pratical Questions ###"
      ],
      "metadata": {
        "id": "tnbUtQ9YDGrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0TFYb9zJDNMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k3zToDAzDNKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1m4YJOTbDNI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "It4f_rIWDNG8",
        "outputId": "3be3eede-24de-4ee8-c495-150f5a279629"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GOJSfdCyEJNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TXb0JW1bEI0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy with L1 regularization: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA-Hp32mEIyI",
        "outputId": "09998162-cb6b-4e18-c9c8-ce5b7c23b562"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 regularization: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KXyTpQixEQMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g9dsWPIqEW6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RhaAWnbrEW4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9mhmgoQXEW1p",
        "outputId": "e361167b-4a1c-4564-bf80-3640b32e6889"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.5, 2.4, 3.7, 1. ],\n",
              "       [6.3, 2.8, 5.1, 1.5],\n",
              "       [6.4, 3.1, 5.5, 1.8],\n",
              "       [6.6, 3. , 4.4, 1.4],\n",
              "       [7.2, 3.6, 6.1, 2.5],\n",
              "       [5.7, 2.9, 4.2, 1.3],\n",
              "       [7.6, 3. , 6.6, 2.1],\n",
              "       [5.6, 3. , 4.5, 1.5],\n",
              "       [5.1, 3.5, 1.4, 0.2],\n",
              "       [7.7, 2.8, 6.7, 2. ],\n",
              "       [5.8, 2.7, 4.1, 1. ],\n",
              "       [5.2, 3.4, 1.4, 0.2],\n",
              "       [5. , 3.5, 1.3, 0.3],\n",
              "       [5.1, 3.8, 1.9, 0.4],\n",
              "       [5. , 2. , 3.5, 1. ],\n",
              "       [6.3, 2.7, 4.9, 1.8],\n",
              "       [4.8, 3.4, 1.9, 0.2],\n",
              "       [5. , 3. , 1.6, 0.2],\n",
              "       [5.1, 3.3, 1.7, 0.5],\n",
              "       [5.6, 2.7, 4.2, 1.3],\n",
              "       [5.1, 3.4, 1.5, 0.2],\n",
              "       [5.7, 3. , 4.2, 1.2],\n",
              "       [7.7, 3.8, 6.7, 2.2],\n",
              "       [4.6, 3.2, 1.4, 0.2],\n",
              "       [6.2, 2.9, 4.3, 1.3],\n",
              "       [5.7, 2.5, 5. , 2. ],\n",
              "       [5.5, 4.2, 1.4, 0.2],\n",
              "       [6. , 3. , 4.8, 1.8],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [6. , 2.2, 4. , 1. ],\n",
              "       [5.4, 3. , 4.5, 1.5],\n",
              "       [6.2, 3.4, 5.4, 2.3],\n",
              "       [5.5, 2.3, 4. , 1.3],\n",
              "       [5.4, 3.9, 1.7, 0.4],\n",
              "       [5. , 2.3, 3.3, 1. ],\n",
              "       [6.4, 2.7, 5.3, 1.9],\n",
              "       [5. , 3.3, 1.4, 0.2],\n",
              "       [5. , 3.2, 1.2, 0.2],\n",
              "       [5.5, 2.4, 3.8, 1.1],\n",
              "       [6.7, 3. , 5. , 1.7],\n",
              "       [4.9, 3.1, 1.5, 0.2],\n",
              "       [5.8, 2.8, 5.1, 2.4],\n",
              "       [5. , 3.4, 1.5, 0.2],\n",
              "       [5. , 3.5, 1.6, 0.6],\n",
              "       [5.9, 3.2, 4.8, 1.8],\n",
              "       [5.1, 2.5, 3. , 1.1],\n",
              "       [6.9, 3.2, 5.7, 2.3],\n",
              "       [6. , 2.7, 5.1, 1.6],\n",
              "       [6.1, 2.6, 5.6, 1.4],\n",
              "       [7.7, 3. , 6.1, 2.3],\n",
              "       [5.5, 2.5, 4. , 1.3],\n",
              "       [4.4, 2.9, 1.4, 0.2],\n",
              "       [4.3, 3. , 1.1, 0.1],\n",
              "       [6. , 2.2, 5. , 1.5],\n",
              "       [7.2, 3.2, 6. , 1.8],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5.1, 3.5, 1.4, 0.3],\n",
              "       [4.4, 3. , 1.3, 0.2],\n",
              "       [6.3, 2.5, 4.9, 1.5],\n",
              "       [6.3, 3.4, 5.6, 2.4],\n",
              "       [4.6, 3.4, 1.4, 0.3],\n",
              "       [6.8, 3. , 5.5, 2.1],\n",
              "       [6.3, 3.3, 6. , 2.5],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [6.1, 2.9, 4.7, 1.4],\n",
              "       [6.5, 2.8, 4.6, 1.5],\n",
              "       [6.2, 2.8, 4.8, 1.8],\n",
              "       [7. , 3.2, 4.7, 1.4],\n",
              "       [6.4, 3.2, 5.3, 2.3],\n",
              "       [5.1, 3.8, 1.6, 0.2],\n",
              "       [6.9, 3.1, 5.4, 2.1],\n",
              "       [5.9, 3. , 4.2, 1.5],\n",
              "       [6.5, 3. , 5.2, 2. ],\n",
              "       [5.7, 2.6, 3.5, 1. ],\n",
              "       [5.2, 2.7, 3.9, 1.4],\n",
              "       [6.1, 3. , 4.6, 1.4],\n",
              "       [4.5, 2.3, 1.3, 0.3],\n",
              "       [6.6, 2.9, 4.6, 1.3],\n",
              "       [5.5, 2.6, 4.4, 1.2],\n",
              "       [5.3, 3.7, 1.5, 0.2],\n",
              "       [5.6, 3. , 4.1, 1.3],\n",
              "       [7.3, 2.9, 6.3, 1.8],\n",
              "       [6.7, 3.3, 5.7, 2.1],\n",
              "       [5.1, 3.7, 1.5, 0.4],\n",
              "       [4.9, 2.4, 3.3, 1. ],\n",
              "       [6.7, 3.3, 5.7, 2.5],\n",
              "       [7.2, 3. , 5.8, 1.6],\n",
              "       [4.9, 3.6, 1.4, 0.1],\n",
              "       [6.7, 3.1, 5.6, 2.4],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [6.9, 3.1, 4.9, 1.5],\n",
              "       [7.4, 2.8, 6.1, 1.9],\n",
              "       [6.3, 2.9, 5.6, 1.8],\n",
              "       [5.7, 2.8, 4.1, 1.3],\n",
              "       [6.5, 3. , 5.5, 1.8],\n",
              "       [6.3, 2.3, 4.4, 1.3],\n",
              "       [6.4, 2.9, 4.3, 1.3],\n",
              "       [5.6, 2.8, 4.9, 2. ],\n",
              "       [5.9, 3. , 5.1, 1.8],\n",
              "       [5.4, 3.4, 1.7, 0.2],\n",
              "       [6.1, 2.8, 4. , 1.3],\n",
              "       [4.9, 2.5, 4.5, 1.7],\n",
              "       [5.8, 4. , 1.2, 0.2],\n",
              "       [5.8, 2.6, 4. , 1.2],\n",
              "       [7.1, 3. , 5.9, 2.1]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aButNNA9Er25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2PPF1pJuEr0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = LogisticRegression(penalty='elasticnet', l1_ratio=0.5, solver='saga', max_iter=500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy with Elastic Net regularization: {accuracy:.4f}\")\n",
        "print(\"Coefficients:\", model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tGCLZ2jdEryc",
        "outputId": "d542eee1-d920-4f6b-9c1a-1f5ea7f8073d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with Elastic Net regularization: 1.0000\n",
            "Coefficients: [[ 0.29355428  1.69356567 -2.35455275 -0.56815234]\n",
            " [ 0.          0.          0.         -0.45412829]\n",
            " [-0.93085072 -0.9135452   2.51058723  2.00308203]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vLxfwvilE3ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qtT7ebOvE3g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    'solver': ['saga']  # saga supports l1, l2 and elasticnet penalties\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=500), param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid.best_params_\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Test Accuracy with best params: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8w5BHCmoE8VQ",
        "outputId": "895f2f0e-a6ca-4505-a552-42f62fe3371c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 1, 'penalty': 'l2', 'solver': 'saga'}\n",
            "Test Accuracy with best params: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "20 fits failed out of a total of 60.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1203, in fit\n",
            "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
            "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.34285714 0.86666667        nan 0.94285714 0.93333333        nan\n",
            " 0.97142857 0.98095238        nan 0.96190476 0.96190476        nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QSTs2XTyFHVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q3CP07QmFHSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy?\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "X = data.drop('target_column', axis=1)\n",
        "y = data['target_column']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "collapsed": true,
        "id": "6IglmocTFHyP",
        "outputId": "13b3c92f-32db-4983-fa2a-1febfc1640c8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'your_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c167ef7efdaa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'your_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'target_column'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YiQ2HgUlFctr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_u43ZzA8Fcri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform\n",
        "import numpy as np\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "param_dist = {\n",
        "    'C': uniform(0.01, 10),\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    'solver': ['saga'],\n",
        "    'l1_ratio': np.linspace(0, 1, 10)  # Only used with elasticnet penalty\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    LogisticRegression(max_iter=500),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    random_state=42\n",
        ")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Test Accuracy with best params: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BV4kOfjJFcpJ",
        "outputId": "bf85634e-d015-4c1f-df36-2fcad0eec665"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': np.float64(1.0097491581800289), 'l1_ratio': np.float64(0.7777777777777777), 'penalty': 'l1', 'solver': 'saga'}\n",
            "Test Accuracy with best params: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g1NU0NXjFsu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a3oQbFA_Fssg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "X, y = make_classification(n_samples=500, n_features=5, n_classes=2, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "\n",
        "plt.title('Confusion Matrix for Logistic Regression')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "LXSdpomaFsp7",
        "outputId": "6c5083c0-51a9-4d3b-9b60-d93aa44c204d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ele7q8PfF41e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ba8Q7oGOF4zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2,\n",
        "                           weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model_no_weights = LogisticRegression(max_iter=500)\n",
        "model_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = model_no_weights.predict(X_test)\n",
        "\n",
        "model_weights = LogisticRegression(class_weight='balanced', max_iter=500)\n",
        "model_weights.fit(X_train, y_train)\n",
        "y_pred_weights = model_weights.predict(X_test)\n",
        "\n",
        "print(\"Without class weights:\")\n",
        "print(classification_report(y_test, y_pred_no_weights))\n",
        "\n",
        "print(\"With class weights:\")\n",
        "print(classification_report(y_test, y_pred_weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrOaB7BhF4xO",
        "outputId": "53ec8ecb-5182-4115-e8cc-599f952e2c90"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without class weights:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.99      0.96       267\n",
            "           1       0.84      0.48      0.62        33\n",
            "\n",
            "    accuracy                           0.93       300\n",
            "   macro avg       0.89      0.74      0.79       300\n",
            "weighted avg       0.93      0.93      0.93       300\n",
            "\n",
            "With class weights:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.90      0.94       267\n",
            "           1       0.50      0.82      0.62        33\n",
            "\n",
            "    accuracy                           0.89       300\n",
            "   macro avg       0.74      0.86      0.78       300\n",
            "weighted avg       0.92      0.89      0.90       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MCZ-ZTIlGDos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hn5MR5M_GDmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model_no_scaling = LogisticRegression(max_iter=500)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=500)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy without scaling: {acc_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with scaling: {acc_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bCcO5jdHGDkT",
        "outputId": "d6557038-baf6-45e2-8a9b-e2b05cb651fe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.9708\n",
            "Accuracy with scaling: 0.9825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Af6xzIqJGQDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cAdVHul9GQBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = LogisticRegression(C=0.5, max_iter=500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy with C=0.5: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCCJktpnGQbd",
        "outputId": "a3572498-20bb-4637-ef0c-794a78946ef6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with C=0.5: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ti8vB5SwGa9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "raXBDHHJGa6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score.\\\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0m84S4lGbRk",
        "outputId": "abb5a97e-84ff-4835-cb0b-24d9b478b985"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uVVPkwP_Gph1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rLnODkrsGpac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "results = {}\n",
        "\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=500)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results[solver] = acc\n",
        "\n",
        "for solver, acc in results.items():\n",
        "    print(f\"Solver: {solver}, Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoIvGAH6GpN_",
        "outputId": "2252f685-1374-4bc7-d091-10b51a24bcf4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solver: liblinear, Accuracy: 0.9778\n",
            "Solver: saga, Accuracy: 1.0000\n",
            "Solver: lbfgs, Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8nWIQQi9G1jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GKfZE8IMG1hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model_raw = LogisticRegression(max_iter=500)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "acc_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=500)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy on raw data: {acc_raw:.4f}\")\n",
        "print(f\"Accuracy on standardized data: {acc_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sk36UaRBG1ff",
        "outputId": "dc592832-a1dd-48c2-90fd-fff25aed1419"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data: 0.9708\n",
            "Accuracy on standardized data: 0.9825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2S9uQCvBHFwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LENWFn1nHFuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=500), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best C: {grid.best_params_['C']}\")\n",
        "print(f\"Best cross-validation score: {grid.best_score_:.4f}\")\n",
        "print(f\"Test set accuracy: {grid.score(X_test, y_test):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DyVid7d1HFsL",
        "outputId": "2d9b7a74-7671-415f-a484-90a8caacc3ca"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C: 100\n",
            "Best cross-validation score: 0.9622\n",
            "Test set accuracy: 0.9591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MrKhlvxDHRKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iAcJzgC6HRIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "joblib.dump(model, 'logistic_model.joblib')\n",
        "\n",
        "loaded_model = joblib.load('logistic_model.joblib')\n",
        "\n",
        "predictions = loaded_model.predict(X_test)\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI-RVtNyHRF9",
        "outputId": "eb0e7106-19b7-4a9d-fd9e-198bc4f0d611"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
            " 0 0 0 2 1 1 0 0]\n"
          ]
        }
      ]
    }
  ]
}